{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea2.ipynb","provenance":[{"file_id":"1kfzWUw6a18_nWZgZ-qPj-kD89G-kDiSW","timestamp":1644192136628}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0LpamkCgFzBw"},"source":["###**Paso 1** "]},{"cell_type":"markdown","source":["### Sigue este Notebook para completar tu tarea 2"],"metadata":{"id":"U7MLEjLUf3Mc"}},{"cell_type":"markdown","metadata":{"id":"fResLg1R9AMW"},"source":["**Paso 1. Importamos el módulo y subimos el texto que vamos a procesar**"]},{"cell_type":"code","metadata":{"id":"81FPOlOJeXL0"},"source":["import nltk # importamos el módulo"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOkhqY2L3Yes"},"source":["Descargamos el paquete 'punkt' desde NLTK. Esto será necesario para evitar mensajes de error más abajo. "]},{"cell_type":"code","metadata":{"id":"8UZNHcNx3WDV"},"source":[" nltk.download('punkt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocesamiento"],"metadata":{"id":"-SNshJkk9Nbu"}},{"cell_type":"code","metadata":{"id":"pl3OmVXI8rlb"},"source":["archivo = open(\"/content/???.txt\", mode='r', encoding='utf-8') # COMPLETAR"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCQ0h7XE8rld"},"source":["texto = archivo.read() # leemos el archivo y lo montamos en la memoria mediante una nueva variable llamada texto\n","archivo.close() # cerramos el archivo anterior"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKvkheWb8rld"},"source":["print(texto[:500], \"...\") # imprimimos el texto que subimos a la memoria de Python"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hHH0oBQ8y6O"},"source":["texto = texto.lower()\n","print(texto[:500], \"...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKUpv2Tc8y6Q"},"source":["tokens = nltk.word_tokenize(texto)\n","print(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVgL-Q0d87N-"},"source":["words = [word for word in tokens if word.isalpha()] # aquí utilizamos un for loop: va token por token y revisa si es .isalpha=TRUE. Si es así lo deja en la lista. \n","print(words) # revisamos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhIHJCm887OA"},"source":["nltk.download('stopwords') # ojo: debemos instalar primero esta funcionalidad\n","from nltk.corpus import stopwords # importamos\n","print(stopwords.words('spanish')) # imprimimos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-tcYIWZ9DtZ"},"source":["stop_words = set(stopwords.words('spanish')) # creamos una variables con todos los stopwords para el idioma español\n","words = [w for w in words if not w in stop_words] # volvemos a hacer un for loop para que deje en mi lista todos los tokens que no son stopwords. \n","print(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOCjPwcu9KhW"},"source":["**1. Unigrams**"]},{"cell_type":"code","metadata":{"id":"CEr057gn9KhY"},"source":["myTokenFD = nltk.FreqDist(words) # utilizamos la función nltk.FreqDist() en nuestra lista de tokens\n","print(myTokenFD) # imprimimos el resultado"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmk1XIHq9KhZ"},"source":["for token in list(myTokenFD.items()): # en este for loop vamos token por token en nuestra lista mediante la función .items().\n","  print(token[0], token[1]) # luego imprimimos el token en la posición [0], y el token en la segunda posición [1]. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwuCT4iq9KhZ"},"source":["print(\"Estas son las 10 palabras más frecuentes en los datos\") # título de nuestra tabla\n","print(\"Token:\\t\\tFrecuencia\") # subtítulo separados por tabulaciones (\\t)\n","for i in myTokenFD.most_common(10): # imprimimos mediante un for loop los 20 tokens más comunes. \n","    print(\"{}\\t\\t{}\".format(i[0],i[1])) # imprimimos 3 datos separados por tabulaciones"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dtmkYZ5i9dda"},"source":["**2. Bigrams**"]},{"cell_type":"code","metadata":{"id":"6yHH2Kjv9dda"},"source":["myTokenBigrams = nltk.ngrams(words, 2) # creamos una variable con secuencias de dos tokens\n","bigrams = list(myTokenBigrams) # convertimos la variable en una lista\n","myBigramFD = nltk.FreqDist(bigrams) # creamos el perfil de frecuencia\n","print(myBigramFD) # Type-Token Ratio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GjGgHDK9ddb"},"source":["print(\"Estos son los 10 bigrams más frecuentes de los datos:\")\n","print(\"Bigram:\\t\\tFrecuencia\")\n","for ngram in list(myBigramFD.most_common(10)): # imprimimos elegantemente\n","    print(\" \".join(ngram[0]), \"\\t\", ngram[1])\n","print(\"Type/Token ratio for bigrams:\", myBigramFD)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Owtjykoe9ddb"},"source":["**2.3. Gráficos**"]},{"cell_type":"code","metadata":{"id":"byrzPTr49ddb"},"source":["import matplotlib.pyplot as plt # importamos Matplotlib, el módulo de gráficos de Python por excelencia\n","import seaborn as sns # importamos seaborn, una libreria de Matplotlib para visualización de datos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phBsrmhz9ddb"},"source":["common_bigrams = [bigram[0] for bigram in myBigramFD.most_common(10)] # Asignamos una variable que contenga el primer item de la lista de bigram mediante un for loop. Sólo sacamos los 10 más comunes\n","common_bigrams = [' '.join(i) for i in common_bigrams] # como un bigram se separa por comas, creamos un for loop para unir los bigrams por espacios. Esto sólo se utiliza para unir los bigrams\n","bigram_counts = [bigram[1] for bigram in myBigramFD.most_common(10)] # sacamos el conteo de los 10 bigrams más comunes mediante un for loop.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jw2ROZJ19ddb"},"source":["fig2 = plt.figure(figsize=(20,7)) # creamos la configuración de nuestro gráfico\n","sns.barplot(x= common_bigrams, y= bigram_counts) # asignamos las variables para el eje vertical y el eje horizontal\n","plt.title('Los 10 bigrams más utilizados en los datos') # le ponemos un título a nuestro gráfico"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Paso 2** "],"metadata":{"id":"MEDW28Yf9uxR"}},{"cell_type":"code","source":["!pip install -U spacy"],"metadata":{"id":"1uZVnERlw2bd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Debemos descargar el modelo mediano de español. Este modelo es más grande y útil para esta actividad.\n","!python -m spacy download es_core_news_md\n","nlp = spacy.load(\"es_core_news_md\")"],"metadata":{"id":"_iGzHa1Ec3hV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pegamos los texto entre las comillas\n","ensayo = nlp(\"\")\n","beginner = nlp(\"\")\n","intermediate = nlp(\"\")\n","advanced = nlp(\"\")"],"metadata":{"id":"qWNh8kj4Y3-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(???, \"<->\", ???, ???.similarity(???)) #COMPLETAR"],"metadata":{"id":"-GqC3Patxdtv"},"execution_count":null,"outputs":[]}]}